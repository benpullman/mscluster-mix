{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import sys\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class Cluster:\n",
    "    def __init__(self,number,a_graph,b_graph):\n",
    "        self.number = number\n",
    "        self.a_graph = a_graph\n",
    "        self.b_graph = b_graph\n",
    "        self.a_index = {edge: i for i,edge in enumerate(a_graph.edges())}\n",
    "        self.b_index = {edge: i for i,edge in enumerate(b_graph.edges())}\n",
    "    def nodes(self):\n",
    "        return self.a_graph.nodes()\n",
    "    def a_edges(self,nbunch = None):\n",
    "        return self.a_graph.edges(nbunch)\n",
    "    def b_edges(self,nbunch = None):\n",
    "        return self.b_graph.edges(nbunch)\n",
    "    def split_at(self,e1,e2,l):\n",
    "        try:\n",
    "            self.a_graph.remove_edge(e1,e2)\n",
    "        except:\n",
    "            try:\n",
    "                self.a_graph.remove_edge(e2,e1)\n",
    "            except:\n",
    "                return []\n",
    "        new_a_graphs = nx.connected_component_subgraphs(self.a_graph)\n",
    "        new_clusters = []\n",
    "        for i,a in enumerate(new_a_graphs):\n",
    "            b = nx.Graph()\n",
    "            for n1 in a:\n",
    "                for n2 in a:\n",
    "                    if n1 != n2:\n",
    "                        try:\n",
    "                            b.add_edge(n1,n2,weight=self.b_graph[n1][n2]['weight'])\n",
    "                            b.add_edge(n2,n1,weight=self.b_graph[n2][n1]['weight'])\n",
    "                        except:\n",
    "                            pass\n",
    "            new_clusters.append(Cluster(str(self.number) + \".\" + chr(l + i + 97),a,b))\n",
    "        return new_clusters\n",
    "    def split_within(self,e1,e2):\n",
    "        try:\n",
    "            self.a_graph.remove_edge(e1,e2)\n",
    "        except:\n",
    "            try:\n",
    "                self.a_graph.remove_edge(e2,e1)\n",
    "            except:\n",
    "                return\n",
    "        new_a_graphs = nx.connected_component_subgraphs(self.a_graph)\n",
    "        new_clusters = [] \n",
    "        b = nx.Graph()\n",
    "        for i,a in enumerate(new_a_graphs):\n",
    "            for n1 in a:\n",
    "                for n2 in a:\n",
    "                    if n1 != n2:\n",
    "                        b.add_edge(n1,n2,weight=self.b_graph[n1][n2]['weight'])\n",
    "                        b.add_edge(n2,n1,weight=self.b_graph[n2][n1]['weight'])\n",
    "        self.b_graph = b\n",
    "    def weight(self):\n",
    "        return avg_cc_weight(self.b_graph)\n",
    "    def id_as(self):\n",
    "        result_ids = set()\n",
    "        for n in self.nodes():\n",
    "            try:\n",
    "                result_ids.add(ids[n])\n",
    "            except:\n",
    "                pass\n",
    "        return result_ids\n",
    "    def lowest_weight(self):\n",
    "        weight_edges = []\n",
    "        for e in self.b_edges():\n",
    "            edge_weight = float(self.b_graph[e[0]][e[1]]['weight'])\n",
    "            weight_edges.append((edge_weight,e))\n",
    "        weight_edges.sort(key=lambda x: x[0])\n",
    "        if len(weight_edges) == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return weight_edges[0]\n",
    "        \n",
    "\n",
    "def pickle_clusters(clusters,filename = \"clusters.p\"):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(clusters,f)\n",
    "\n",
    "def load_clusters(filename = \"clusters.p\"):\n",
    "    clusters = []\n",
    "    with open(filename, \"rb\") as f:\n",
    "        clusters = pickle.load(f)\n",
    "    return clusters\n",
    "\n",
    "def pickle_ids(ids,filename = \"ids.p\"):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(ids,f)\n",
    "\n",
    "def load_ids(filename = \"ids.p\"):\n",
    "    ids = {}\n",
    "    with open(filename, \"rb\") as f:\n",
    "        ids = pickle.load(f)\n",
    "    return ids\n",
    "\n",
    "def generate_identification(filename):\n",
    "    identifcation = {}\n",
    "    cluster_number = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            e = line.replace(\"\\n\",\"\").rsplit(\",\")\n",
    "            identifcation[e[0]] = e[1]\n",
    "            cluster_number[e[0]] = int(e[2])\n",
    "    return identifcation, cluster_number\n",
    "\n",
    "def generate_a_edges(filename):\n",
    "    G=nx.Graph()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            e = line.replace(\"\\n\",\"\").rsplit(\",\")\n",
    "            G.add_edge(e[0],e[1])\n",
    "    return G\n",
    "\n",
    "def generate_b_edges(filename):\n",
    "    G=nx.Graph()\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            e = line.replace(\"\\n\",\"\").rsplit(\",\")\n",
    "            G.add_edge(e[0],e[1],weight=e[2])\n",
    "    return G\n",
    "\n",
    "def cc_weights(cc):\n",
    "    weights = []\n",
    "    for e in cc.edges():\n",
    "        edge_weight = float(cc[e[0]][e[1]]['weight'])\n",
    "        if edge_weight < 1:\n",
    "            weights.append(edge_weight)\n",
    "    return weights\n",
    "\n",
    "def avg_cc_weight(cc):\n",
    "    weights = cc_weights(cc)\n",
    "    weight_result = 1\n",
    "    try:\n",
    "        weight_result = sum(weights)/len(weights)\n",
    "    except:\n",
    "        pass\n",
    "    return weight_result\n",
    "\n",
    "def cluster_ids(cc,ids):\n",
    "    id_set = []\n",
    "    for n in cc.nodes():\n",
    "        try:\n",
    "            id_set.append(ids[n])\n",
    "        except:\n",
    "            pass\n",
    "    return {s for s in id_set}\n",
    "\n",
    "def mix(cc,ids):\n",
    "    c_ids = cluster_ids(cc,ids)\n",
    "    if 'PEPTIDE' in c_ids:\n",
    "        return len(c_ids) > 2\n",
    "    else:\n",
    "        return len(c_ids) > 1\n",
    "\n",
    "def an_id(cc,ids):\n",
    "    c_ids = cluster_ids(cc,ids)\n",
    "    if 'PEPTIDE' in c_ids:\n",
    "        return len(c_ids)>1\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def all_id(cc,ids):\n",
    "    c_ids = cluster_ids(cc,ids)\n",
    "    return not 'PEPTIDE' in c_ids\n",
    "\n",
    "def link_edges(a_clusters,b_clusters):\n",
    "    clusters = []\n",
    "    for cluster_id in a_clusters:\n",
    "        try:\n",
    "            clusters.append(Cluster(cluster_id,a_clusters[cluster_id],b_clusters[cluster_id]))\n",
    "        except:\n",
    "            pass\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def seperate_mixtures(graph):\n",
    "    mix_graphs = {}\n",
    "    m = 0\n",
    "    s = 0\n",
    "    p = 0\n",
    "    sp = 0\n",
    "    e = 0\n",
    "    for cluster in nx.connected_component_subgraphs(graph):\n",
    "        clust = cluster_ids(cluster,ids)\n",
    "        if len(clust)==1 and 'PEPTIDE' in clust:\n",
    "            s += 1\n",
    "        elif len(clust)==1 and not 'PEPTIDE' in clust:\n",
    "            p += 1\n",
    "        elif len(clust)==2 and 'PEPTIDE' in clust:\n",
    "            sp += 1\n",
    "        elif mix(cluster,ids):\n",
    "            try:\n",
    "                key = cluster_number[cluster.nodes()[0]]\n",
    "                mix_graphs[key] = cluster\n",
    "            except:\n",
    "                pass\n",
    "            m += 1\n",
    "            # if avg_cc_weight(cluster) < .7:\n",
    "            #     for edge in cluster.edges():\n",
    "            #         print(edge[0] + \" \" + edge[1] + \": \" + cluster[edge[0]][edge[1]]['weight'])\n",
    "        else:\n",
    "            e += 1\n",
    "    # print(\"Not id'd: \" + str(s))\n",
    "    # print(\"Mixtures: \" + str(m))\n",
    "    # print(\"Pure: \" + str(p))\n",
    "    # print(\"Maybe pure: \" + str(sp))\n",
    "    # print(\"Error: \" + str(e))\n",
    "    return mix_graphs\n",
    "\n",
    "clusters = []\n",
    "try:\n",
    "    a_graph = generate_a_edges(sys.argv[1])\n",
    "    b_graph = generate_b_edges(sys.argv[2])\n",
    "    ids, cluster_number = generate_identification(sys.argv[3])\n",
    "    mixture_a = seperate_mixtures(a_graph)\n",
    "    mixture_b = seperate_mixtures(b_graph)\n",
    "    clusters = link_edges(mixture_a,mixture_b)\n",
    "    pickle_clusters(clusters)\n",
    "    pickle_ids(ids)\n",
    "except:\n",
    "    clusters = load_clusters()\n",
    "    ids = load_ids()\n",
    "\n",
    "# for n in a_graph.nodes():\n",
    "#     try:\n",
    "#         m += 1\n",
    "#         print(ids[n])\n",
    "#     except:\n",
    "#         s += 1\n",
    "\n",
    "\n",
    "c = clusters[1]\n",
    "E = np.zeros((len(c.b_edges()),len(c.a_edges())),dtype=np.int)\n",
    "# print(E)\n",
    "# print(c.nodes())\n",
    "# # print(c.lowest_weight())\n",
    "# # print(c.id_as())\n",
    "# # for cluster in c.split_at(c.a_edges()[0][0], c.a_edges()[0][1]):\n",
    "# #     print(cluster.id_as())\n",
    "# #     # print(cluster.nodes())\n",
    "# #     # print(cluster.b_edges())\n",
    "# #     # print(cluster.weight())\n",
    "# #     print(cluster.lowest_weight())\n",
    "\n",
    "\n",
    "# print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dfs(clust, node, E, in_edge):\n",
    "    E_v = np.zeros((len(clust.b_edges())),dtype=np.int)\n",
    "    a_idx = None\n",
    "    if in_edge:\n",
    "        try:\n",
    "            a_idx = clust.a_index[in_edge]\n",
    "        except:\n",
    "            a_idx = clust.a_index[(in_edge[1],in_edge[0])]\n",
    "    clust.a_graph.node[node]['visited'] = True\n",
    "    for b_edge in clust.b_edges(node):\n",
    "        try:\n",
    "            edge_idx = clust.b_index[b_edge]\n",
    "        except:\n",
    "            edge_idx = clust.b_index[(b_edge[1],b_edge[0])]\n",
    "        E_v[edge_idx] = 1\n",
    "    for n in clust.a_graph.neighbors(node):\n",
    "        if not clust.a_graph.node[n]['visited']:\n",
    "            try:\n",
    "                edge_idx = clust.a_index[(node,n)]\n",
    "            except:\n",
    "                edge_idx = clust.a_index[(n,node)]\n",
    "            E = dfs(clust,n,E,(node,n))\n",
    "            E_v = combine(E_v, E[:,edge_idx].T)\n",
    "    if in_edge:\n",
    "        E[:,a_idx] = E_v.T\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xor(x,y):\n",
    "    if x == 1 and y == 1:\n",
    "        return 0\n",
    "    elif x == 0 and y == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def combine(E_x,E_y):\n",
    "    E_z = np.zeros((len(E_x)),dtype=np.int)\n",
    "    for i in range(0,len(E_x)):\n",
    "        E_z[i] = xor(E_x[i],E_y[i])\n",
    "    return E_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_E(clust,cc,E):\n",
    "    weight_tuple = []\n",
    "    for a_edge in cc.a_edges():\n",
    "        weights = []\n",
    "        try:\n",
    "            a_idx = clust.a_index[a_edge]\n",
    "        except:\n",
    "            a_idx = clust.a_index[(a_edge[1],a_edge[0])]\n",
    "        for b_edge in cc.b_edges():\n",
    "            try:\n",
    "                b_idx = clust.b_index[b_edge]\n",
    "            except:\n",
    "                b_idx = clust.b_index[(b_edge[1],b_edge[0])]\n",
    "            if E[(b_idx,a_idx)] == 1:\n",
    "                try:\n",
    "                    weights.append(float(clust.b_graph[b_edge[0]][b_edge[1]]['weight']))\n",
    "                except:\n",
    "                    weights.append(float(clust.b_graph[b_edge[1]][b_edge[0]]['weight']))\n",
    "        try:\n",
    "            weight_tuple.append((a_edge,sum(weights)/len(weights)))\n",
    "        except:\n",
    "            weight_tuple.append((a_edge,0))\n",
    "    min_weight = sorted(weight_tuple,key=lambda x: x[1])[0]\n",
    "    return min_weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_min_cut(cluster,E,theta):\n",
    "    ccs_below, ccs_above = cc_below_threshold([cluster],theta)\n",
    "    clusters = ccs_above\n",
    "    while len(ccs_below) > 0:\n",
    "        clust = ccs_below.pop()\n",
    "#         print(clust.nodes())\n",
    "#         print(len(clust.nodes()))\n",
    "#         print(len(clust.b_edges()))\n",
    "        edge = score_E(cluster,clust,E)\n",
    "        new_clusters = clust.split_at(edge[0],edge[1],0)\n",
    "        ccs_below.append(new_clusters[0])\n",
    "        ccs_below.append(new_clusters[1])\n",
    "        ccs, ccs_above = cc_below_threshold(ccs_below,theta)\n",
    "        clusters = clusters + ccs_above\n",
    "        ccs_below = ccs\n",
    "    return clusters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cc_below_threshold(ccs,theta):\n",
    "    ccs_below = []\n",
    "    ccs_above = []\n",
    "    for cc in ccs:\n",
    "        if cc.weight()<theta:\n",
    "            ccs_below.append(cc)\n",
    "        else:\n",
    "            ccs_above.append(cc)\n",
    "    return ccs_below, ccs_above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "250\n",
      "500\n",
      "750\n",
      "1000\n",
      "1250\n",
      "1500\n",
      "1750\n",
      "2000\n",
      "2250\n",
      "2500\n",
      "2750\n",
      "3000\n",
      "3250\n",
      "3500\n",
      "3750\n",
      "4000\n",
      "4250\n",
      "4500\n",
      "4750\n",
      "5000\n",
      "5250\n",
      "5500\n",
      "5750\n",
      "6000"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for c in clusters:\n",
    "#     print(\"---\")\n",
    "#     print(len(c.nodes()))\n",
    "    if total%250 == 0:\n",
    "        print(total)\n",
    "    E = np.zeros((len(c.b_index),len(c.a_index)),dtype=np.int)\n",
    "    for a in c.a_graph:\n",
    "        c.a_graph.node[a][\"visited\"] = False\n",
    "    E = dfs(c,c.nodes()[0],E,None)\n",
    "    new_c = find_min_cut(c,E,.8)\n",
    "    total += 1\n",
    "    if effectiveness(new_c):\n",
    "        correct += 1\n",
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00522_D02_P003811_B0L_A00_R1.mzXML:8141'"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.nodes()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2] + []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def effectiveness(clusters):\n",
    "    for cluster in clusters:\n",
    "        if mix(cluster,ids):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spectra_recovered(clusters):\n",
    "    total_spectra = 0\n",
    "    for cluster in clusters:\n",
    "        \n",
    "        if mix(cluster,ids):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
